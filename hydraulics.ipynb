{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137fbae3-e229-4e31-b275-22298c5ac054",
   "metadata": {},
   "source": [
    "Firstly, import necessary dependencies to the project. In this first version, we can start our study on some fundamental algorithms in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad33dc-8ad9-4340-b936-81b7a674c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923245b-d170-4302-a66a-7f9fb3c4c8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9142a88-1cdd-482f-858a-6dd9f303dd36",
   "metadata": {},
   "source": [
    "Give configuration parameters for data, model, and also algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c25ba87-9711-4c60-86c1-c7717335bef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        'predict_mode': 'all', # 'all' or 'single'\n",
    "        'n_sample': 1000,\n",
    "        'input_path': 'data/',\n",
    "        'model': 'GradientBoosting',\n",
    "        'params_model': {\n",
    "            'loss': 'log_loss',\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'random_state': 0,\n",
    "            'learning_rate': 0.7,\n",
    "            'subsample': 0.7,\n",
    "            'min_samples_split': 20,\n",
    "            'min_samples_leaf': 20\n",
    "            \n",
    "        },\n",
    "        're_train': False,\n",
    "        're_balancing_features': True\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffd9d4-5ee6-4dba-a200-3a89263e6f42",
   "metadata": {},
   "source": [
    "Load raw data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef795a-3c91-430f-b259-891b67bfe4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = pd.read_csv(parameters['input_path'] + \"FS1.txt\", sep=\"\\t\", header = None)\n",
    "dps = pd.read_csv(parameters['input_path'] + \"PS2.txt\", sep=\"\\t\", header = None)\n",
    "dpf = pd.read_csv(parameters['input_path'] + \"profile.txt\", sep=\"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f569a-3898-427c-8a26-833328215687",
   "metadata": {
    "tags": []
   },
   "source": [
    "The raw data is numerical type, so we can consider its distribution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a8c266b-dcf6-4941-8930-554d8dc3306e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2090248, 2665817, 4865653,  921079, 3589301,  396344,    6817,\n",
       "           3184,    7109,    7448]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Histogram for data in FS1.txt\n",
    "tmp = np.concatenate([np.array(dfs[i].tolist()) for i in dfs.dtypes.index])\n",
    "tmp\n",
    "counts, bins = np.histogram(x)\n",
    "\n",
    "counts, bins\n",
    "\n",
    "#plt.stairs(counts, bins)\n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ad0c3c9-ee7f-401a-9158-952a9ee78792",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2090248, 2665817, 4865653,  921079, 3589301,  396344,    6817,\n",
       "           3184,    7109,    7448]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Histogram for data in PS2.txt\n",
    "tmp = np.concatenate([np.array(dfs[i].tolist()) for i in dfs.dtypes.index])\n",
    "tmp\n",
    "counts, bins = np.histogram(x)\n",
    "counts, bins\n",
    "\n",
    "#plt.stairs(counts, bins)\n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa173b9-7a14-4736-a2bf-98885290d336",
   "metadata": {
    "tags": []
   },
   "source": [
    "The distribution of data give the possibility to discretize them. In this version, we use Kmeans as a model to categorize the data into features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2aa102f0-e4fa-4226-8cbc-f0b426c80baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans = dict()\n",
    "for i in dfs.dtypes.index:\n",
    "    tmp = np.column_stack((np.array(dfs[i].tolist()),np.array([0]*len(dfs))))\n",
    "    kmeans[\"fs_\" + str(i)] = KMeans(n_clusters=10, random_state=0).fit(tmp)\n",
    "\n",
    "\n",
    "\n",
    "for i in dps.dtypes.index:\n",
    "    tmp = np.column_stack((np.array(dps[i].tolist()),np.array([0]*len(dps))))\n",
    "    kmeans[\"ps_\" + str(i)] = KMeans(n_clusters=5, random_state=0).fit(tmp)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "with open('models/embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeans, file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ed44e662-dfb8-4928-82a1-f053d31d0a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "#Feature processing for data in FS1.txt\n",
    "for i in dfs.dtypes.index:\n",
    "    tmp = np.column_stack((np.array(dfs[i].tolist()),np.array([0]*len(dfs))))\n",
    "    x.append(kmeans[\"fs_\" + str(i)].predict(tmp))\n",
    "\n",
    "\n",
    "#Feature processing for data in PS2.txt\n",
    "for i in dps.dtypes.index:\n",
    "    tmp = np.column_stack((np.array(dps[i].tolist()),np.array([0]*len(dps))))\n",
    "    x.append(kmeans[\"ps_\" + str(i)].predict(tmp))\n",
    "        \n",
    "x = np.column_stack(x)\n",
    "#get label from profile.txt\n",
    "y = np.array(dpf[4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5111190d-6799-412e-9c19-7a2332e4badc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1449,  756]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution of label\n",
    "np.unique(y, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c28d2d-9096-46d0-aca9-9f14b791af33",
   "metadata": {},
   "source": [
    "We start the model training. Here some of our remarques\n",
    "\n",
    "1. We user GradientBoostingClassifier with log_loss as loss to train the data.\n",
    "2. Due to the imbalance of the data (e.g. label), we suggest doing a re-sampling to handle it. In this study, we simplify by using imblearn.RandomUnderSampler\n",
    "3. Turn the parameters of GradientBoostingClassifier to have better performance and reduce overfitting.\n",
    "4. If you want to use a deeplearning model, we suggest applying at least one-hot encoder on the features to a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01e76e5b-ece5-41bf-a7b5-8c1a5289fbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       149\n",
      "           1       0.96      0.89      0.92        72\n",
      "\n",
      "    accuracy                           0.95       221\n",
      "   macro avg       0.95      0.93      0.94       221\n",
      "weighted avg       0.95      0.95      0.95       221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "if parameters['re_balancing_features']:\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    X_train, Y_train = rus.fit_resample(X_train, Y_train) \n",
    "    \n",
    "clf = GradientBoostingClassifier(**parameters['params_model']).fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, y_pred))\n",
    "\n",
    "\n",
    "#Save the model\n",
    "with open('models/model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7563de-d911-4ce9-b54e-1742163c04a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
